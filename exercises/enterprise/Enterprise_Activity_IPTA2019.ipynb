{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Enterprise` Activity for IPTA 2019 Student Week $-$ Pune, India (NCRA/TIFR)\n",
    "\n",
    "## Using enterprise to do Bayesian analysis\n",
    "\n",
    "\n",
    "`ENTERPRISE` (Enhanced Numerical Toolbox Enabling a Robust PulsaR Inference SuitE) is a pulsar timing analysis code, aimed at noise analysis, gravitational-wave searches, and timing model analysis. This tutorial is based on the enterprise's documentation, https://enterprise.readthedocs.io/en, the worksheets on IPTA GitHub, https:/github.com/ipta/gwa_tutorials, and the tutorials of last IPTA meeting, https://github.com/ipta/ipta-2018-workshop/tree/master/gw_detection.\n",
    "\n",
    "You should have learned how to use `TempoNest` and got some experience of Bayesian analysis in pulsar timing. Usually we sample the parameter space with existing Bayesian sampling softwares, like `MultiNest` used by `TempoNest`. We provide the sampler with prior distribution and likelihood function, and the sampler will give us the posterior distrubution. The key part of `TempoNest` and `enterprise` is how to calculate the likelihood. We specify the data, the prior distribution of parameters, and the components of signal model, `TempoNest` will construct the likelihood function and pass it to `Multinest`. `Enterprise` does similar things, but in a more flexible way. It provides the ability to add complicated signals to PTA models, like GW signal, and you can even write your own signal easily. \n",
    "\n",
    "In this notebook, we will start with noise analysis of single pulsar. Then you wil learn how to search GW background and GW from individual source. You may know that Bayesian inference is computationally expensive, especially when using PTA to search GWs. So we can only get the results quickly for simple case of single-pulsar noise analysis (also only with limited parameters and data points.) We will show how to post-process the results for this case. For the GW problem, we only show how to setup the analysis and get the likelihood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T17:42:33.177433Z",
     "start_time": "2018-05-15T17:42:31.657125Z"
    }
   },
   "outputs": [],
   "source": [
    "#from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import os, glob, json \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as sl\n",
    "\n",
    "import enterprise\n",
    "from enterprise.pulsar import Pulsar\n",
    "import enterprise.signals.parameter as parameter\n",
    "from enterprise.signals import utils\n",
    "from enterprise.signals import signal_base\n",
    "from enterprise.signals import selections\n",
    "from enterprise.signals.selections import Selection\n",
    "from enterprise.signals import white_signals\n",
    "from enterprise.signals import gp_signals\n",
    "from enterprise.signals import deterministic_signals\n",
    "import enterprise.constants as const\n",
    "import enterprise_extensions\n",
    "from enterprise_extensions import models, model_utils\n",
    "\n",
    "import corner\n",
    "from PTMCMCSampler.PTMCMCSampler import PTSampler as ptmcmc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Noise analysis of single pulsar\n",
    "We start with noise analysis of single pulsar, as you have done using `TempoNest`. The likelihood function is determined by the data and the signal model. First, we load the pulsar timing data into `Pulsar` object in `enterprise`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in `Pulsar` class\n",
    "`Enterprise` uses a specific `Pulsar` object to store all of the relevant pulsar information (i.e. TOAs, residuals, error bars, flags, etc) from the timing package like `tempo2` or `PINT`. This class is instantiated with a par and a tim file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = enterprise.__path__[0] + '/datafiles/mdc_open1/'\n",
    "par = datadir+'J0030+0451.par'\n",
    "tim = datadir+'J0030+0451.tim'\n",
    "#datadir = enterprise.__path__[0] + '/datafiles/ng9/'\n",
    "#par=datadir+'J0613-0200_NANOGrav_9yv1.gls.par'\n",
    "#tim=datadir+'J0613-0200_NANOGrav_9yv1.tim'\n",
    "psr = Pulsar(par, tim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `psr` contains all data needed for the analysis. You no longer need to reference the `par` and `tim` files after this cell. You can check the content of `psr`, such as name, TOAs and timing residuals. The simulated data have small TOA errors and strong red noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(psr.name)\n",
    "plt.errorbar(psr.toas/86400,psr.residuals*1e6,yerr=psr.toaerrs*1e6,fmt='.');\n",
    "plt.xlabel('t(MJD)')\n",
    "plt.ylabel('res $(\\mu s)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal model\n",
    "Second, we need to set up the signal model. The basic model-building steps in `enterprise` is as following, \n",
    "* Define `parameters` and priors;\n",
    "* Set up the `signals` these `parameters` belong to;\n",
    "* Define the `model` by summing the individual `signals`;\n",
    "* Define `PTA` by initializing the signal model with a `Pulsar` object.\n",
    "\n",
    "For this simulated data, only white noise (measurement noise) and red noise are needed. However, we will also show how to create the parameters and signals for other noise processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create parameters\n",
    "`Parameters` are set by specifying a prior distribution (i.e., uniform, log-uniform, etc.). The parameters include Efac, Equad and Ecorr for white noise, amplitude and spectra index for red noise and DM noise with power-law spectra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# white noise parameters\n",
    "efac = parameter.Uniform(0.01, 10.0)\n",
    "equad = parameter.Uniform(-8.5, -5)\n",
    "ecorr = parameter.Uniform(-8.5, -5)\n",
    "\n",
    "# red noise parameters\n",
    "log10_A = parameter.Uniform(-20, -11)\n",
    "gamma = parameter.Uniform(0, 7)\n",
    "\n",
    "# dm-variation parameters\n",
    "log10_A_dm = parameter.Uniform(-20, -11)\n",
    "gamma_dm = parameter.Uniform(0, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create signals\n",
    "\n",
    "Then we create the signal components respectively. \n",
    "* `White noise`: The white noise signals corresponding to Efac, Equad and Ecorr are in the `white signals` module with name of `MeasurementNoise`, `EquadNoise` and `EcorrKernelNoise`. Note that different backends have different values of white noise parameters, so we also use `selection` class to split the data by backends and get the white noise parameters for every backend.\n",
    "* `Red noise`: The red noise signal is in the `gp_signals` module. We need the spectra and basis function to construct red noise. Here we use power-law spectra and Fourier basis. First we create the spectra using `powerlaw` function in `utils` module, and then use the `FourierBasisGP` function to create red noise. We need to specify the frequency sampling by the time span of the data and the number of frequency componnents .\n",
    "* `DM noise`: The DM noise is similar to red noise, except that it is frequency dependent. We use the `createfourierdesignmatrix_dm` function to create the basis of DM noise.\n",
    "* `timing model`: We also include timing model, which will be marginalized analytically to make the noise parameter estimation robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define selection by observing backend\n",
    "selection = selections.Selection(selections.by_backend)\n",
    "\n",
    "# special selection for ECORR only use wideband NANOGrav data\n",
    "selection2 = selections.Selection(selections.nanograv_backends)\n",
    "\n",
    "# white noise\n",
    "ef = white_signals.MeasurementNoise(efac=efac, selection=selection)\n",
    "eq = white_signals.EquadNoise(log10_equad=equad, selection=selection)\n",
    "ec = white_signals.EcorrKernelNoise(log10_ecorr=ecorr, selection=selection2)\n",
    "\n",
    "# find the maximum time span to set red-noise/DM-variation frequency sampling\n",
    "tmin = psr.toas.min()\n",
    "tmax = psr.toas.max()\n",
    "Tspan = np.max(tmax) - np.min(tmin)\n",
    "\n",
    "# red noise (powerlaw with 30 frequencies)\n",
    "pl = utils.powerlaw(log10_A=log10_A, gamma=gamma)\n",
    "rn = gp_signals.FourierBasisGP(spectrum=pl, components=30, Tspan=Tspan)\n",
    "\n",
    "# DM-variations (powerlaw with 30 frequencies)\n",
    "dm_basis = utils.createfourierdesignmatrix_dm(nmodes=30, Tspan=Tspan)\n",
    "dm_pl = utils.powerlaw(log10_A=log10_A_dm, gamma=gamma_dm)\n",
    "dm_gp = gp_signals.BasisGP(dm_pl, dm_basis, name='dm_gp')\n",
    "\n",
    "# timing model\n",
    "tm = gp_signals.TimingModel(use_svd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Piece the full model together\n",
    "\n",
    "We add the signal components together to create the signal model. The combination of signal model and data to calculate the likelihood  is achieved in the `PTA` class. So we initialize the `model` calss with `psr` object, and pass it to the `PTA` class to create `pta` object (with one pulsar only). We add ef, rn and tm for this simulated data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize a single-pulsar pta model\n",
    "# model=ef + eq + ec + rn + dm\n",
    "model = ef + rn  + tm\n",
    "    \n",
    "pta = signal_base.PTA(model(psr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have finished constructing the signal model. We can look into the `pta` object to see which parameters we are going to be searching over with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in pta.params:\n",
    "    print(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start the MCMC chain at a random point in parameter space. We accomplish this by setting up a parameter dictionary using the name and sample methods for each Parameter. We can check the likelihood and prior at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = {par.name: par.sample() for par in pta.params}\n",
    "print(pta.get_lnlikelihood(xs));\n",
    "print(pta.get_lnprior(xs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up sampler \n",
    "The rest of the analysis here is dependent on the sampling method and not on `enterprise` itself. Traditionally `enterprise` uses PTMCMCSampler package for sampling, but you can also use other samplers, such as `MultiNest` used in `TempoNest`. \n",
    "For PTMCMCSampler, as many others, it requires a function to compute the log-likelihood and log-prior given a vector of parameters. Here, these are supplied by `PTA` class as `pta.get_lnlikelihood` and `pta.get_lnprior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial jump covariance matrix\n",
    "x0 = np.hstack(p.sample() for p in pta.params)\n",
    "ndim = len(x0)\n",
    "cov = np.diag(np.ones(ndim) * 0.01**2) # helps to tune MCMC proposal distribution\n",
    "\n",
    "# where chains will be written to\n",
    "outdir = './chains/noise_run_{}/'.format(str(psr.name))\n",
    "\n",
    "# sampler object\n",
    "sampler = ptmcmc(ndim, pta.get_lnlikelihood, pta.get_lnprior, cov,\n",
    "                 outDir=outdir, \n",
    "                 resume=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample the parameter space\n",
    "\n",
    "The time taken by sampling depends on your laptop. You will see printout like \"Finished aa percent in bb s\". You can estimate the time needed, which should be several minutes. If it is too long, you can stop sampling by \"Kernel->Interrupt\" and jump to Part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sampler for N steps\n",
    "#N=int(1e6)\n",
    "N = int(5e4)\n",
    "\n",
    "# SCAM = Single Component Adaptive Metropolis\n",
    "# AM = Adaptive Metropolis\n",
    "# DE = Differential Evolution\n",
    "## You can keep all these set at default values\n",
    "sampler.sample(x0, N, SCAMweight=30, AMweight=15, DEweight=50, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple post-processing\n",
    "Now we read the output of sampling and plot the posterior distribution of parameters. The histograms show the marginalized posterior distrubution of every parameter, and the contour plots show the 2-d posterior distribution of parameter pairs, from which we can see the correlation between parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = np.loadtxt(outdir + 'chain_1.txt')\n",
    "burn = int(0.25 * chain.shape[0])\n",
    "corner.corner(chain[burn:,:-4],labels=pta.param_names,levels=[0.68,0.95]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Stochastic gravitational wave background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GWB analysis is very similar to the noise analysis above, except that we need to analyze multiple pulsars simultaneously to search for spatial correlation between pulsars. Another main difference is that we add GWB signal to the signal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get par, tim, and noise files\n",
    "Here we collect the tim and par files as well as noise files. These are the same par, tim, and noise files used in the NANOGrav 9-year analysis papers. We need noise files because we want to fix white noise parameters at the values from single-pulsar noise analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = enterprise.__path__[0] + '/datafiles/ng9/'\n",
    "\n",
    "parfiles = sorted(glob.glob(datadir + '/*.par'))\n",
    "timfiles = sorted(glob.glob(datadir + '/*.tim'))\n",
    "noisefiles = sorted(glob.glob(datadir + '/*noise.txt'))\n",
    "\n",
    "# 18 pulsars used in 9 year analysis\n",
    "p9 = np.loadtxt(datadir+'/9yr_pulsars.txt', dtype=str)#'S42')\n",
    "\n",
    "# filter\n",
    "parfiles = [x for x in parfiles if x.split('/')[-1].split('_')[0] in p9]\n",
    "timfiles = [x for x in timfiles if x.split('/')[-1].split('_')[0] in p9]\n",
    "noisefiles = [x for x in noisefiles if x.split('/')[-1].split('_')[0] in p9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you execute the following cell, you may get warnings like `WARNING: Could not find pulsar distance for PSR ...`. Don't worry! This is expected, and fine. Not all pulsars have well constrained distances, and will be set to `1 kpc` with a `20%` uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psrs = []\n",
    "for p, t in zip(parfiles, timfiles):\n",
    "    psr = Pulsar(p, t, ephem='DE421')\n",
    "    psrs.append(psr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal model\n",
    "The basic difference in signal model is that we need to create the GWB signal. The GWB signal is also stochastic with power-law spectra, but with spatial correlation described by Hellings-Downs curve. Note that GWB parameters are common between different pulsars. We initialize GW parameters with name, so that these parameters will be shared among pulsars, rather than create one set of parameters for every pulsar.\n",
    "The spectra index of GWB is 13/3, so we fix this parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GW parameters (initialize with names here to use parameters in common across pulsars)\n",
    "log10_A_gw = parameter.LinearExp(-18,-12)('log10_A_gw')\n",
    "gamma_gw = parameter.Constant(4.33)('gamma_gw')\n",
    "\n",
    "# gwb\n",
    "cpl = utils.powerlaw(log10_A=log10_A_gw, gamma=gamma_gw)\n",
    "orf = utils.hd_orf()\n",
    "gw = gp_signals.FourierBasisCommonGP(cpl, orf, components=30, Tspan=Tspan, name='gw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the signal model is created in a similar way to noise analysis, but with multiple pulsars. In search for GW, we usually fix the white noise parameters at the value from single-pulsar noise analysis. We can also add the modelling of errors in solar system ephemeris to the signal model, using `PhysicalEphemerisSignal` function in `deterministic_signals` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the maximum time span to set GW frequency sampling\n",
    "tmin = [p.toas.min() for p in psrs]\n",
    "tmax = [p.toas.max() for p in psrs]\n",
    "Tspan = np.max(tmax) - np.min(tmin)\n",
    "\n",
    "# selection class to break white noise by backend\n",
    "selection = selections.Selection(selections.by_backend)\n",
    "\n",
    "##### parameters and priors #####\n",
    "\n",
    "# white noise parameters\n",
    "# since we are fixing these to values from the noise file we set\n",
    "# them as constant parameters\n",
    "efac = parameter.Constant()\n",
    "equad = parameter.Constant()\n",
    "ecorr = parameter.Constant()\n",
    "\n",
    "# red noise parameters \n",
    "log10_A = parameter.LinearExp(-20,-12)\n",
    "gamma = parameter.Uniform(0,7)\n",
    "\n",
    "\n",
    "##### Set up signals #####\n",
    "\n",
    "# white noise\n",
    "ef = white_signals.MeasurementNoise(efac=efac, selection=selection)\n",
    "eq = white_signals.EquadNoise(log10_equad=equad, selection=selection)\n",
    "ec = white_signals.EcorrKernelNoise(log10_ecorr=ecorr, selection=selection)\n",
    "\n",
    "# red noise (powerlaw with 30 frequencies)\n",
    "pl = utils.powerlaw(log10_A=log10_A, gamma=gamma)\n",
    "rn = gp_signals.FourierBasisGP(spectrum=pl, components=30, Tspan=Tspan)\n",
    "\n",
    "# timing model\n",
    "tm = gp_signals.TimingModel()\n",
    "\n",
    "# to add solar system ephemeris modeling...\n",
    "eph = deterministic_signals.PhysicalEphemerisSignal(use_epoch_toas=True)\n",
    "\n",
    "# full model is sum of components\n",
    "model = ef + eq + ec + rn + tm + eph + gw\n",
    "\n",
    "# intialize PTA\n",
    "pta = signal_base.PTA([model(psr) for psr in psrs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set values of white noise parameters from noise files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert `PAL2` noise files to `enterprise` parameter dictionaries.\n",
    "def get_noise_from_pal2(noisefile):\n",
    "    psrname = noisefile.split('/')[-1].split('_noise.txt')[0]\n",
    "    fin = open(noisefile, 'r')\n",
    "    lines = fin.readlines()\n",
    "    params = {}\n",
    "    for line in lines:\n",
    "        ln = line.split()\n",
    "        if 'efac' in line:\n",
    "            par = 'efac'\n",
    "            flag = ln[0].split('efac-')[-1]\n",
    "        elif 'equad' in line:\n",
    "            par = 'log10_equad'\n",
    "            flag = ln[0].split('equad-')[-1]\n",
    "        elif 'jitter_q' in line:\n",
    "            par = 'log10_ecorr'\n",
    "            flag = ln[0].split('jitter_q-')[-1]\n",
    "        elif 'RN-Amplitude' in line:\n",
    "            par = 'log10_A'\n",
    "            flag = ''\n",
    "        elif 'RN-spectral-index' in line:\n",
    "            par = 'gamma'\n",
    "            flag = ''\n",
    "        else:\n",
    "            break\n",
    "        if flag:\n",
    "            name = [psrname, flag, par]\n",
    "        else:\n",
    "            name = [psrname, par]\n",
    "        pname = '_'.join(name)\n",
    "        params.update({pname: float(ln[1])})\n",
    "    return params\n",
    "\n",
    "params = {}\n",
    "for nfile in noisefiles:\n",
    "    params.update(get_noise_from_pal2(nfile))\n",
    "    \n",
    "pta.set_default_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have set up the model. We can check the parameters and the likelihood to see if everything works correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in pta.params:\n",
    "    print(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = {par.name: par.sample() for par in pta.params}\n",
    "print(pta.get_lnlikelihood(xs));\n",
    "print(pta.get_lnprior(xs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic idea of GWB analysis\n",
    "The rest of work is left to the Bayesian sampler. This part is computationally expensive, and is impossible to finish here. We just introduce some basic ideas of GWB analysis. \n",
    "* To answer whether we have detected GWB, we need compare models with and without GWB signal. We can only claim detection of GWB when the model with GWB has significantly larger Bayesian evidence. \n",
    "* If we don't detect GWB, we can set an upper limit on its amplitude. Note that for the amplitude, we use uniform prior in upper limit analysis, and log-uniform prior in searching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Gravitational wave from single source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GWB is thought to be the most likely source to be detected by PTA. However, It's also possible to detect continuous gravitational wave (CGW) from individual supermassive blackhole binary, if the signal is strong enough. The CGW signal is deterministic, which depends on the mass, orbital parameters, and position of the binary. CGW analysis uses the same data as GWB analysis, so we only need to deal with the signal model. In the following, we first create CGW parameters, then calculate the CGW waveform, and create the CGW signal. Note that we are initializing them with names here so that they will be shared across all pulsars in the PTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous GW parameters\n",
    "\n",
    "cos_gwtheta = parameter.Uniform(-1, 1)('cos_gwtheta')\n",
    "gwphi = parameter.Uniform(0, 2*np.pi)('gwphi')\n",
    "log10_mc = parameter.Uniform(7, 10)('log10_mc')\n",
    "log10_fgw = parameter.Uniform(-10,-7)('log10_fgw')\n",
    "phase0 = parameter.Uniform(0, 2*np.pi)('phase0')\n",
    "psi = parameter.Uniform(0, np.pi)('psi')\n",
    "cos_inc = parameter.Uniform(-1, 1)('cos_inc')\n",
    "\n",
    "log10_h = parameter.LinearExp(-18, -11)('log10_h')\n",
    "\n",
    "# define CGW waveform and signal\n",
    "cw_wf = models.cw_delay(cos_gwtheta=cos_gwtheta, gwphi=gwphi, log10_mc=log10_mc, \n",
    "                     log10_h=log10_h, log10_fgw=log10_fgw, phase0=phase0, \n",
    "                     psi=psi, cos_inc=cos_inc)\n",
    "cw = models.CWSignal(cw_wf, psrTerm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace the GWB signal in the previous part with CGW signal, and create the model and PTA object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full model is sum of components\n",
    "model = ef + eq + ec + rn + tm + cw\n",
    "\n",
    "# intialize PTA\n",
    "pta = signal_base.PTA([model(psr) for psr in psrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set values of white noise parameters\n",
    "params = {}\n",
    "for nfile in noisefiles:\n",
    "    params.update(get_noise_from_pal2(nfile))\n",
    "    \n",
    "pta.set_default_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the case of GWB analysis, we also stop at checking the parameters and evaluating the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in pta.params:\n",
    "    print(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = {par.name: par.sample() for par in pta.params}\n",
    "print(pta.get_lnlikelihood(xs));\n",
    "print(pta.get_lnprior(xs));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic idea in CGW analysis\n",
    "* If we want to quantify whether we have detected CGW, we need to do model selection by comparing the Bayesian evidence of models with and without CGW signal. \n",
    "* If we don't detect the signal, we can also set an upper limit on the CGW amplitude. The upper limit is usually given as a function of frequency. To do this, we fix the frequency parameter, and perform Bayesian sampling for the rest of parameters to get the upper limit of GW amplitude. Repeat this process for a series of frequencies, we can get the CGW upper limit at different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "743px",
    "left": "0px",
    "right": "1458px",
    "top": "107px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
